{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fe7d11",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch\n",
    "\n",
    "__Notebook Author__: Ramtin Moslemi\n",
    "\n",
    "## Notebook Objectives\n",
    "\n",
    "\n",
    "In this notebook we are going to implement and train a neural network from scratch using only numpy!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b09e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tgrange\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(losses):\n",
    "    #plot the loss\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, kept_classes):\n",
    "    dim = len(kept_classes)\n",
    "    labels = [class_names[i] for i in kept_classes]\n",
    "    #plot the confusion matrix\n",
    "    conf_mat = confusion_matrix(y_true,y_pred)\n",
    "    norm_conf_mat = conf_mat/np.sum(conf_mat,axis=1)\n",
    "    #plot the matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(norm_conf_mat)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predictions\")\n",
    "    plt.ylabel(\"Labels\")\n",
    "    plt.xticks(range(dim), labels,rotation = 45)\n",
    "    plt.yticks(range(dim),labels)\n",
    "    plt.colorbar()\n",
    "    # Put number of each cell in plot\n",
    "    for i in range(dim):\n",
    "        for j in range(dim):\n",
    "            c = conf_mat[j,i]\n",
    "            color = \"black\" if c > 500 else \"white\"\n",
    "            ax.text(i, j, str(int(c)), va = \"center\", ha = \"center\", color= color)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_data(filter_classes):\n",
    "    fashion_mnist = fetch_openml(\"Fashion-MNIST\",parser='auto')\n",
    "    x,y = fashion_mnist[\"data\"], fashion_mnist['target'].astype(int)\n",
    "    # Remove classes\n",
    "    filtered_indices = np.isin(y, filter_classes)\n",
    "    x, y = fashion_mnist[\"data\"], fashion_mnist[\"target\"].astype(int)\n",
    "    # Normalize the pixels to be in [-1, +1] range\n",
    "    x = ((x / 255.)-0.5)* 2\n",
    "    removed_class_count = 0\n",
    "    for i in range(10): # Fix the labels\n",
    "        if i in filter_classes and removed_class_count != 0:\n",
    "            y[y == i] = i - removed_class_count\n",
    "        elif i not in filter_classes:\n",
    "            removed_class_count +=1\n",
    "    # DO the train-test split\n",
    "    return train_test_split(x,y,test_size=10_000)\n",
    "\n",
    "\n",
    "def onehot_encoder (y, num_labels):\n",
    "    one_hot = np.zeros(shape= (y.size, num_labels), dtype=int)\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "def plot_batch_size(vanila, stochastic, mini_batch):\n",
    "    fig, axes = plt.subplots(2 , 2)\n",
    "    # Plot the loss\n",
    "    axes[0, 0].plot(vanila[0], label = \"Gradient Descent\")\n",
    "    axes[0, 0].plot(stochastic[0], label = \"Stochastic Gradient Descent\")\n",
    "    axes[0, 0].plot(mini_batch[0], label = \"Mini-Batch Gradient Descent\")\n",
    "    axes[0, 0].set_xlabel(\"Epoch\"), axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].set_title(\"Training Loss\"), axes[0, 0].legend()\n",
    "    # Plot the accuracy\n",
    "    axes[0, 1].plot(vanila[2], label = \"Gradient Descent\")\n",
    "    axes[0, 1].plot(stochastic[2], label = \"Stochastic Gradient Descent\")\n",
    "    axes[0, 1].plot(mini_batch[2], label = \"Mini-Batch Gradient Descent\")\n",
    "    axes[0, 1].set_xlabel(\"Epoch\"), axes[0, 1].set_ylabel(\"Loss\")\n",
    "    axes[0, 1].set_title(\"Training Loss\"), axes[0, 1].legend()\n",
    "    # Plot SGD batch loss\n",
    "    axes[1, 0].plot(stochastic[1], label = \"Stochastic Gradient Descent\")\n",
    "    axes[1, 0].set_xlabel(\"Batch\"), axes[1, 0].set_ylabel(\"Loss\")\n",
    "    axes[1, 0].set_title(\"Stochastic Gradient Descent\")\n",
    "    # Plot MBGD batch loss\n",
    "    axes[1, 1].plot(stochastic[1], label = \"Mini-Batch Gradient Descent\")\n",
    "    axes[1, 1].set_xlabel(\"Batch\"), axes[1, 1].set_ylabel(\"Loss\")\n",
    "    axes[1, 1].set_title(\"Mini-Batch Gradient Descent\")\n",
    "\n",
    "    fig.set_size_inches(16, 12)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67838ca6",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac07c8",
   "metadata": {},
   "source": [
    "## Abstract Layer Class\n",
    "\n",
    "The `Layer` class serves as an abstract base class for all layers in the network. It includes placeholder methods:\n",
    "- `forward`: The forward pass computes the output of the layer given an input.\n",
    "- `backward`: The backward pass computes the gradients with respect to the input and parameters.\n",
    "- `step`: Updates the layer parameters (weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49221922",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.inp = None\n",
    "        self.out = None\n",
    "\n",
    "    def __call__(self, inp: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(inp)\n",
    "    \n",
    "    def forward(self, inp: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, up_grad: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step (self, lr: float) -> None:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
